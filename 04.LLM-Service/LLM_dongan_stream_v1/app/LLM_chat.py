#!/usr/bin/env python3
# ìµœì í™”ëœ LLM_chat.py - ì¡¸ìŒìš´ì „ ë°©ì§€ LLM ìœ í‹¸ë¦¬í‹°
import os, json, re, time
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List

class LLMChat:
    def __init__(self):
        self.setup_config()
        self.setup_openai()
        self.load_payloads()
        
    def setup_config(self):
        """í™˜ê²½ë³€ìˆ˜ ë° ì„¤ì •"""
        # ê°„ë‹¨í•œ dotenv ë¡œë”©
        try:
            from dotenv import load_dotenv
            for env_file in [".env", ".env.example"]:
                if Path(env_file).exists():
                    load_dotenv(env_file, override=env_file==".env")
        except ImportError:
            pass
            
        self.config = {
            'api_key': os.getenv("OPENAI_API_KEY"),
            'model': os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            'timeout': 10,
            'max_tokens': 150
        }
        
    def setup_openai(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        self.client = None
        if self.config['api_key']:
            try:
                import openai
                self.client = openai.OpenAI(api_key=self.config['api_key'])
            except Exception as e:
                print(f"OpenAI setup failed: {e}")
                
        print(f"LLM configured: {bool(self.client)}")
        
    def load_payloads(self):
        """ê¸°ë³¸ í˜ì´ë¡œë“œ ë° ì™¸ë¶€ íŒŒì¼ ë¡œë“œ"""
        # ì••ì¶•ëœ ê¸°ë³¸ í˜ì´ë¡œë“œ
        self.payloads = {
            'ì •ìƒ': {'announcement': 'ì •ìƒ ìƒíƒœì…ë‹ˆë‹¤.', 'question': ''},
            'ì˜ì‹¬ê²½ê³ ': {'announcement': 'ì£¼ì˜! ì¡¸ìŒ ì‹ í˜¸ ê°ì§€', 'question': ''},
            'ì§‘ì¤‘ëª¨ë‹ˆí„°ë§': {'announcement': 'ìƒíƒœ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.', 'question': ''},
            'ê°œì„ ': {'announcement': 'ìƒíƒœê°€ ë‚˜ì•„ì¡Œì–´ìš”.', 'question': ''},
            'L1': {'announcement': 'ì¡¸ìŒì´ ì§€ì†ë©ë‹ˆë‹¤. íœ´ì‹ì„ ê¶Œì¥í•´ìš”.', 'question': 'ì§€ê¸ˆ ë‚ ì§œê°€ ëª‡ ì¼ì´ì£ ?'},
            'L2': {'announcement': 'ê°•í•œ ì¡¸ìŒ ì‹ í˜¸ì…ë‹ˆë‹¤. ì°½ë¬¸ ì—´ê¸° ê¶Œì¥.', 'question': 'ì„œìš¸ ë‹¤ìŒ ë„ì‹œëŠ”?'},
            'L3': {'announcement': 'ê³ ìœ„í—˜ ì¡¸ìŒ ìƒíƒœì…ë‹ˆë‹¤.', 'question': '10ì—ì„œ 1ê¹Œì§€ ê±°ê¾¸ë¡œ ë§í•´ë³´ì„¸ìš”.'},
            'FAILSAFE': {'announcement': 'ê³ ìœ„í—˜! ì¦‰ì‹œ ì •ì°¨í•˜ì„¸ìš”.', 'question': ''},
            'íœ´ì‹': {'announcement': 'íœ´ì‹ ëª¨ë“œì…ë‹ˆë‹¤.', 'question': ''}
        }
        
        # ì™¸ë¶€ í˜ì´ë¡œë“œ ë³‘í•©
        payload_file = Path("stage_payloads.json")
        if payload_file.exists():
            try:
                with open(payload_file, encoding='utf-8') as f:
                    external = json.load(f)
                for stage, data in external.items():
                    if stage in self.payloads:
                        self.payloads[stage].update(data)
                    else:
                        self.payloads[stage] = data
                print(f"External payloads merged: {list(external.keys())}")
            except Exception as e:
                print(f"Failed to load external payloads: {e}")
                
    # === Dê°’ -> ë‹¨ê³„ ë³€í™˜ ===
    def determine_stage_from_d(self, D: float) -> str:
        """Dê°’ì„ ë‹¨ê³„ë¡œ ë³€í™˜"""
        try:
            d = float(D)
            stages = [(30,'ì •ìƒ'), (40,'ì˜ì‹¬ê²½ê³ '), (50,'ì§‘ì¤‘ëª¨ë‹ˆí„°ë§'), (60,'ê°œì„ '), 
                     (70,'L1'), (80,'L2'), (90,'L3'), (999,'FAILSAFE')]
            return next((stage for threshold, stage in stages if d <= threshold), 'FAILSAFE')
        except:
            return 'ì •ìƒ'
            
    # === LLM í˜¸ì¶œ ===
    def call_llm(self, stage: str, d: float) -> Optional[Dict[str, str]]:
        """LLM í˜¸ì¶œë¡œ ë™ì  í˜ì´ë¡œë“œ ìƒì„±"""
        if not self.client:
            return None
            
        try:
            prompt = f"""ë‹¹ì‹ ì€ ì¡¸ìŒìš´ì „ ë°©ì§€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¨ê³„: {stage}, ì¡¸ìŒì§€ìˆ˜: {d}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{"announcement": "TTSìš© ì§§ì€ ê²½ê³ ë¬¸ (100ì ì´ë‚´)", "question": "ê²€ì¦ìš© ì§ˆë¬¸ (ë˜ëŠ” ë¹ˆ ë¬¸ìì—´)"}}

í•œêµ­ì–´ë¡œ ì°¨ë¶„í•˜ì§€ë§Œ ë‹¨í˜¸í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""

            response = self.client.chat.completions.create(
                model=self.config['model'],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.config['max_tokens'],
                temperature=0.3,
                timeout=self.config['timeout']
            )
            
            content = response.choices[0].message.content.strip()
            return self.parse_json(content)
            
        except Exception as e:
            print(f"LLM call failed: {e}")
            return None
            
    def parse_json(self, text: str) -> Optional[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ"""
        if not text:
            return None
            
        # ì½”ë“œ íœìŠ¤ ì œê±°
        text = re.sub(r'```.*?```', '', text, flags=re.DOTALL).strip()
        
        # JSON ë¸”ë¡ ì°¾ê¸°
        json_match = re.search(r'\{[^}]*\}', text, re.DOTALL)
        if not json_match:
            return None
            
        try:
            candidate = json_match.group(0)
            return json.loads(candidate)
        except:
            try:
                # ê°„ë‹¨í•œ ìˆ˜ì • ì‹œë„
                fixed = candidate.replace("'", '"')
                fixed = re.sub(r',\s*}', '}', fixed)
                return json.loads(fixed)
            except:
                return None
                
    # === ë©”ì¸ í˜ì´ë¡œë“œ ìƒì„± ===
    def generate_stage_payload(self, stage: str, d: float, prefer_llm: bool = True) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ í˜ì´ë¡œë“œ ìƒì„± (LLM ìš°ì„ , í´ë°± ì§€ì›)"""
        # ê¸°ë³¸ í˜ì´ë¡œë“œ ë³µì‚¬
        base = self.payloads.get(stage, self.payloads['ì •ìƒ']).copy()
        
        # LLM ì¦ê°• ì‹œë„
        if prefer_llm:
            llm_result = self.call_llm(stage, d)
            if llm_result:
                if llm_result.get('announcement'):
                    base['announcement'] = llm_result['announcement'].strip()
                if 'question' in llm_result:
                    base['question'] = llm_result.get('question', '').strip()
                    
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        base.update({
            'stage': stage,
            'd': float(d) if d is not None else None,
            'ts': time.time(),
            'llm_used': prefer_llm and bool(self.client)
        })
        
        return base
        
    # === ì‚¬ìš©ì ë‹µë³€ ê²€ì¦ ===
    def verify_user_answer(self, user_text: str, expected: Optional[List[str]] = None, 
                          regex: Optional[str] = None) -> Tuple[bool, str]:
        """ì‚¬ìš©ì ë‹µë³€ ê²€ì¦"""
        text = (user_text or "").strip().lower()
        if not text:
            return False, "empty"
            
        # ì •í™•í•œ ë‹µë³€ ì²´í¬
        if expected:
            for exp in expected:
                if text == exp.lower() or self.levenshtein(text, exp.lower()) <= 2:
                    return True, "match"
                    
        # ì •ê·œì‹ ì²´í¬
        if regex:
            try:
                if re.search(regex, user_text, re.IGNORECASE):
                    return True, "regex"
            except:
                pass
                
        # ìˆ«ì ë‹µë³€ í—ˆìš©
        if text.isdigit() and regex and r'\d' in regex:
            return True, "numeric"
            
        return False, "no_match"
        
    def levenshtein(self, a: str, b: str) -> int:
        """ê°„ë‹¨í•œ í¸ì§‘ ê±°ë¦¬ ê³„ì‚°"""
        if not a or not b:
            return max(len(a or ""), len(b or ""))
            
        if len(a) < len(b):
            a, b = b, a
            
        if len(b) == 0:
            return len(a)
            
        prev_row = list(range(len(b) + 1))
        for i, a_char in enumerate(a):
            curr_row = [i + 1]
            for j, b_char in enumerate(b):
                insertions = prev_row[j + 1] + 1
                deletions = curr_row[j] + 1
                substitutions = prev_row[j] + (a_char != b_char)
                curr_row.append(min(insertions, deletions, substitutions))
            prev_row = curr_row
            
        return prev_row[-1]

# === ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ë° í¸ì˜ í•¨ìˆ˜ë“¤ ===
_llm_chat = None

def get_llm_chat():
    """LLMChat ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _llm_chat
    if _llm_chat is None:
        _llm_chat = LLMChat()
    return _llm_chat

def generate_stage_payload(stage: str, d: float, prefer_llm: bool = True) -> Dict[str, Any]:
    """í¸ì˜ í•¨ìˆ˜: ë‹¨ê³„ë³„ í˜ì´ë¡œë“œ ìƒì„±"""
    return get_llm_chat().generate_stage_payload(stage, d, prefer_llm)
    
def verify_user_answer(user_text: str, expected_answers: Optional[List[str]] = None, 
                      verify_regex: Optional[str] = None) -> Tuple[bool, str]:
    """í¸ì˜ í•¨ìˆ˜: ì‚¬ìš©ì ë‹µë³€ ê²€ì¦"""
    return get_llm_chat().verify_user_answer(user_text, expected_answers, verify_regex)
    
def determine_stage_from_d(D: float) -> str:
    """í¸ì˜ í•¨ìˆ˜: Dê°’ì„ ë‹¨ê³„ë¡œ ë³€í™˜"""
    return get_llm_chat().determine_stage_from_d(D)

# === í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    print("ğŸ§  LLM_chat í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    llm = get_llm_chat()
    print(f"OpenAI ì—°ê²°: {bool(llm.client)}")
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    test_values = [25, 35, 45, 55, 65, 75, 85, 95]
    for d in test_values:
        stage = determine_stage_from_d(d)
        payload = generate_stage_payload(stage, d, prefer_llm=False)  # LLM ì—†ì´ í…ŒìŠ¤íŠ¸
        print(f"D={d} -> {stage}: {payload['announcement']}")
        
    # LLM í…ŒìŠ¤íŠ¸ (í‚¤ê°€ ìˆëŠ” ê²½ìš°)
    if llm.client:
        print("\nğŸ¤– LLM í…ŒìŠ¤íŠ¸ (L2 ë‹¨ê³„)...")
        llm_payload = generate_stage_payload("L2", 82, prefer_llm=True)
        print(f"LLM ê²°ê³¼: {llm_payload['announcement']}")
        
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")