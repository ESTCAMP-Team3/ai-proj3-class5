{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a301f8c",
   "metadata": {},
   "source": [
    "# 해당 코드들은 자동화 최적화가 되어 있지 않음\n",
    "# 아직은 1개씩만 돌려보고 결과를 확인하기 위한 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d983b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from collections import deque\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91fc970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\dmd\\\\gA\\\\1\\\\s5\\\\gA_1_s5_2019-03-14T14;26;17+01;00_rgb_face.mp4', '.\\\\dmd\\\\gA\\\\2\\\\s5\\\\gA_2_s5_2019-03-13T09;19;23+01;00_rgb_face.mp4', '.\\\\dmd\\\\gA\\\\3\\\\s5\\\\gA_3_s5_2019-03-13T09;36;25+01;00_rgb_face.mp4', '.\\\\dmd\\\\gA\\\\4\\\\s5\\\\gA_4_s5_2019-03-13T10;56;52+01;00_rgb_face.mp4', '.\\\\dmd\\\\gA\\\\5\\\\s5\\\\gA_5_s5_2019-03-13T09;06;49+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\10\\\\s5\\\\gB_10_s5_2019-03-12T10;35;20+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\6\\\\s5\\\\gB_6_s5_2019-03-13T13;37;11+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\7\\\\s5\\\\gB_7_s5_2019-03-13T13;55;52+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\8\\\\s5\\\\gB_8_s5_2019-03-13T14;10;09+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\9\\\\s5\\\\gB_9_s5_2019-03-07T16;31;48+01;00_rgb_face.mp4']\n",
      "['.\\\\dmd\\\\gA\\\\1\\\\s5\\\\gA_1_s5_2019-03-14T14;26;17+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gA\\\\2\\\\s5\\\\gA_2_s5_2019-03-13T09;19;23+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gA\\\\3\\\\s5\\\\gA_3_s5_2019-03-13T09;36;25+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gA\\\\4\\\\s5\\\\gA_4_s5_2019-03-13T10;56;52+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gA\\\\5\\\\s5\\\\gA_5_s5_2019-03-13T09;06;49+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\10\\\\s5\\\\gB_10_s5_2019-03-12T10;35;20+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\10\\\\s5\\\\gB_10_s5_2019-03-13T14;17;28+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\6\\\\s5\\\\gB_6_s5_2019-03-13T13;37;11+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\7\\\\s5\\\\gB_7_s5_2019-03-13T13;55;52+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\8\\\\s5\\\\gB_8_s5_2019-03-13T14;10;09+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\9\\\\s5\\\\gB_9_s5_2019-03-07T16;31;48+01;00_rgb_ann_drowsiness.json']\n"
     ]
    }
   ],
   "source": [
    "target_files = []\n",
    "json_paths = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\"rgb_face.mp4\"):\n",
    "            target_files.append(os.path.join(root, fname))\n",
    "        if fname.endswith(\"drowsiness.json\"):\n",
    "            json_paths.append(os.path.join(root, fname))\n",
    "\n",
    "print(target_files)\n",
    "print(json_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_VIDEO = \"labeled_output.mp4\"              # 출력 비디오 파일명\n",
    "OUTPUT_CSV = \"per_frame_labels.csv\"              # 출력 CSV 파일명\n",
    "\n",
    "CALIB_SECONDS = 2.0       # 초기 캘리브레이션 구간(초)\n",
    "FPS_FALLBACK = 30.0       # FPS 정보가 없을 때 기본값\n",
    "SMOOTH_WIN = 5            # EAR/MAR 이동평균 윈도우\n",
    "BLINK_MAX_FRAMES = 8      # blink로 볼 수 있는 최대 닫힘 프레임 길이\n",
    "YAWN_MIN_FRAMES = 45      # 하품으로 간주할 최소 프레임 길이\n",
    "HAND_MOUTH_DIST_PX = 80   # 손가락 포인트와 입 중심 간 근접 판정 거리(픽셀)\n",
    "\n",
    "# 상태 머신 임계치 비율 (캘리브레이션 결과에 곱해 사용)\n",
    "EYE_CLOSE_RATIO = 0.85    # 눈감김 임계치: EAR_low = median(EAR_calib)*EYE_CLOSE_RATIO\n",
    "EYE_OPEN_RATIO  = 1.05    # 눈뜸 임계치: EAR_high = median(EAR_calib)*EYE_OPEN_RATIO\n",
    "MOUTH_YAWN_RATIO = 1.25   # 하품 임계치: MAR_high = median(MAR_calib)*MOUTH_YAWN_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13200b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaceMesh / Hands 초기화\n",
    "mp_face = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# FaceMesh 눈/입 계산용 랜드마크 인덱스 (MediaPipe FaceMesh)\n",
    "# EAR: (상하 거리 합) / (좌우 거리)  -- 관례적 정의\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]   # [left, top1, top2, right, bottom1, bottom2]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "# MAR: (상하 거리) / (좌우 거리)\n",
    "MOUTH_HORZ = (61, 291)    # 좌우 외측 입꼬리\n",
    "MOUTH_VERT = (13, 14)     # 상하(안쪽 입술 중앙)\n",
    "\n",
    "# 상태 관련 버퍼\n",
    "ear_buf = deque(maxlen=SMOOTH_WIN)\n",
    "mar_buf = deque(maxlen=SMOOTH_WIN)\n",
    "\n",
    "# 눈 상태 머신\n",
    "EYE_OPEN = 0\n",
    "EYE_CLOSE = 1\n",
    "EYE_OPENING = 2\n",
    "EYE_CLOSING = 3\n",
    "EYE_BLINK = 4\n",
    "\n",
    "eye_state = EYE_OPEN    # 초기 가정\n",
    "close_count = 0         # 연속 닫힘 프레임 수(블링크 판정용)\n",
    "\n",
    "# 하품 상태\n",
    "YAWN_NONE = 0\n",
    "YAWN_WITH_HAND = 1\n",
    "YAWN_WITHOUT_HAND = 2\n",
    "\n",
    "yawn_state = YAWN_NONE\n",
    "yawn_count = 0\n",
    "\n",
    "# 캘리브레이션 샘플\n",
    "ear_samples = []\n",
    "mar_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdccdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 상태 및 라벨을 판별하는 함수들 + ROI\n",
    "def euclid(p1, p2):\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye_idx, w, h):\n",
    "    l = landmarks[eye_idx[0]]; r = landmarks[eye_idx[3]]\n",
    "    t1 = landmarks[eye_idx[1]]; t2 = landmarks[eye_idx[2]]\n",
    "    b1 = landmarks[eye_idx[4]]; b2 = landmarks[eye_idx[5]]\n",
    "    l = (l.x*w, l.y*h); r = (r.x*w, r.y*h)\n",
    "    t1 = (t1.x*w, t1.y*h); t2 = (t2.x*w, t2.y*h)\n",
    "    b1 = (b1.x*w, b1.y*h); b2 = (b2.x*w, b2.y*h)\n",
    "    vertical = (euclid(t1, b1) + euclid(t2, b2)) / 2.0\n",
    "    horizontal = euclid(l, r) + 1e-6\n",
    "    return vertical / horizontal\n",
    "\n",
    "def mouth_aspect_ratio(landmarks, w, h):\n",
    "    L = landmarks[MOUTH_HORZ[0]]; R = landmarks[MOUTH_HORZ[1]]\n",
    "    U = landmarks[MOUTH_VERT[0]]; D = landmarks[MOUTH_VERT[1]]\n",
    "    L = (L.x*w, L.y*h); R = (R.x*w, R.y*h)\n",
    "    U = (U.x*w, U.y*h); D = (D.x*w, D.y*h)\n",
    "    horizontal = euclid(L, R) + 1e-6\n",
    "    vertical = euclid(U, D)\n",
    "    return vertical / horizontal, ((L[0]+R[0])/2, (U[1]+D[1])/2)\n",
    "\n",
    "def moving_avg(queue, k):\n",
    "    arr = np.array(queue, dtype=np.float32)\n",
    "    if len(arr)==0: return None\n",
    "    if len(arr) < k:\n",
    "        return float(np.mean(arr))\n",
    "    return float(np.mean(arr[-k:]))\n",
    "\n",
    "def draw_roi(frame, pts, color=(0,255,0), thickness=2):\n",
    "    xs = [p[0] for p in pts]; ys = [p[1] for p in pts]\n",
    "    x1, y1, x2, y2 = int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "def is_hand_near_mouth(hand_landmarks_list, mouth_center, max_dist_px, w, h):\n",
    "    if hand_landmarks_list is None: return False\n",
    "    cx, cy = mouth_center\n",
    "    for hand in hand_landmarks_list:\n",
    "        for lm in hand.landmark:\n",
    "            px, py = lm.x*w, lm.y*h\n",
    "            if euclid((px, py), (cx, cy)) <= max_dist_px:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e041e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = target_files[0]\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if fps <= 1e-2: fps = FPS_FALLBACK\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT)>0 else None\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(\"./output/labeled_output.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "logs = []\n",
    "\n",
    "# 캘리브 구간 프레임 수\n",
    "calib_frames = int(CALIB_SECONDS * fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "112f1674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189.7125 sec\n"
     ]
    }
   ],
   "source": [
    "# MediaPipe 컨텍스트\n",
    "start = time.time()\n",
    "with mp_face.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as face_mesh, mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    # 1) 캘리브레이션\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame_idx += 1\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face_res = face_mesh.process(rgb)\n",
    "\n",
    "        if face_res.multi_face_landmarks:\n",
    "            face_landmarks = face_res.multi_face_landmarks[0].landmark\n",
    "            ear_l = eye_aspect_ratio(face_landmarks, LEFT_EYE, width, height)\n",
    "            ear_r = eye_aspect_ratio(face_landmarks, RIGHT_EYE, width, height)\n",
    "            ear = (ear_l + ear_r) / 2.0\n",
    "            mar, _ = mouth_aspect_ratio(face_landmarks, width, height)\n",
    "            ear_samples.append(ear)\n",
    "            mar_samples.append(mar)\n",
    "\n",
    "        if frame_idx >= calib_frames:\n",
    "            break\n",
    "\n",
    "    # 임계치 계산\n",
    "    if len(ear_samples) >= 5:\n",
    "        ear_med = float(np.median(ear_samples))\n",
    "        EAR_LOW  = ear_med * EYE_CLOSE_RATIO\n",
    "        EAR_HIGH = ear_med * EYE_OPEN_RATIO\n",
    "    else:\n",
    "        EAR_LOW, EAR_HIGH = 0.18, 0.26  # 안전 기본값 (얼굴 크기/거리 따라 다를 수 있음)\n",
    "\n",
    "    if len(mar_samples) >= 5:\n",
    "        mar_med = float(np.median(mar_samples))\n",
    "        MAR_HIGH = mar_med * MOUTH_YAWN_RATIO\n",
    "    else:\n",
    "        MAR_HIGH = 0.6  # 안전 기본값\n",
    "\n",
    "    # 2) 본 처리 루프 (다시 처음부터)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame_idx += 1\n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face_res = face_mesh.process(rgb)\n",
    "        hands_res = hands.process(rgb)\n",
    "\n",
    "        label_id = 0\n",
    "        label_str = \"eyes_state/open\"\n",
    "\n",
    "        if face_res.multi_face_landmarks:\n",
    "            face_landmarks = face_res.multi_face_landmarks[0].landmark\n",
    "\n",
    "            # EAR/MAR\n",
    "            ear_l = eye_aspect_ratio(face_landmarks, LEFT_EYE, width, height)\n",
    "            ear_r = eye_aspect_ratio(face_landmarks, RIGHT_EYE, width, height)\n",
    "            ear = (ear_l + ear_r) / 2.0\n",
    "            mar, mouth_center = mouth_aspect_ratio(face_landmarks, width, height)\n",
    "\n",
    "            ear_buf.append(ear)\n",
    "            mar_buf.append(mar)\n",
    "            ear_s = moving_avg(ear_buf, SMOOTH_WIN)\n",
    "            mar_s = moving_avg(mar_buf, SMOOTH_WIN)\n",
    "\n",
    "            # 눈 상태 머신 업데이트\n",
    "            prev_state = eye_state\n",
    "            if ear_s is None:\n",
    "                eye_state = EYE_OPEN\n",
    "            else:\n",
    "                # opening/closing은 EAR 변화율로 판정(최근 3프레임)\n",
    "                if len(ear_buf) >= 3:\n",
    "                    ear_deriv = ear_buf[-1] - ear_buf[-3]\n",
    "                else:\n",
    "                    ear_deriv = 0.0\n",
    "\n",
    "                is_closed = ear_s < EAR_LOW\n",
    "                is_opened = ear_s > EAR_HIGH\n",
    "\n",
    "                if is_closed:\n",
    "                    if prev_state != EYE_CLOSE:\n",
    "                        eye_state = EYE_CLOSING if ear_deriv < 0 else EYE_CLOSE\n",
    "                    else:\n",
    "                        eye_state = EYE_CLOSE\n",
    "                    close_count += 1\n",
    "                elif is_opened:\n",
    "                    if prev_state != EYE_OPEN:\n",
    "                        eye_state = EYE_OPENING if ear_deriv > 0 else EYE_OPEN\n",
    "                    else:\n",
    "                        eye_state = EYE_OPEN\n",
    "                    # blink 판정: 짧게 닫혔다가 열림\n",
    "                    if 0 < close_count <= BLINK_MAX_FRAMES:\n",
    "                        eye_state = EYE_BLINK\n",
    "                    close_count = 0\n",
    "                else:\n",
    "                    # 임계대역 사이: 파형 기울기 기준으로 opening/closing\n",
    "                    if ear_deriv > 0:\n",
    "                        eye_state = EYE_OPENING\n",
    "                    elif ear_deriv < 0:\n",
    "                        eye_state = EYE_CLOSING\n",
    "                    # 닫힘 유지 중이었다면 카운트 증가\n",
    "                    if prev_state in (EYE_CLOSE, EYE_CLOSING):\n",
    "                        close_count += 1\n",
    "                    else:\n",
    "                        close_count = 0\n",
    "\n",
    "            # 하품 상태 판별\n",
    "            hand_near = is_hand_near_mouth(\n",
    "                hands_res.multi_hand_landmarks if hands_res else None,\n",
    "                mouth_center, HAND_MOUTH_DIST_PX, width, height\n",
    "            )\n",
    "            if mar_s is not None and mar_s > MAR_HIGH:\n",
    "                yawn_count += 1\n",
    "                yawn_state = YAWN_WITH_HAND if hand_near else YAWN_WITHOUT_HAND\n",
    "            else:\n",
    "                yawn_state = YAWN_NONE\n",
    "                yawn_count = 0\n",
    "\n",
    "            # 최종 라벨 우선순위: 하품 > 눈 상태/눈 깜빡임\n",
    "            if yawn_state != YAWN_NONE and yawn_count >= YAWN_MIN_FRAMES:\n",
    "                if yawn_state == YAWN_WITH_HAND:\n",
    "                    label_id, label_str = 5, \"yawning/Yawning with hand\"\n",
    "                else:\n",
    "                    label_id, label_str = 6, \"yawning/Yawning without hand\"\n",
    "            else:\n",
    "                if eye_state == EYE_OPEN:\n",
    "                    label_id, label_str = 0, \"eyes_state/open\"\n",
    "                elif eye_state == EYE_CLOSE:\n",
    "                    label_id, label_str = 1, \"eyes_state/close\"\n",
    "                elif eye_state == EYE_OPENING:\n",
    "                    label_id, label_str = 2, \"eyes_state/opening\"\n",
    "                elif eye_state == EYE_CLOSING:\n",
    "                    label_id, label_str = 3, \"eyes_state/closing\"\n",
    "                elif eye_state == EYE_BLINK:\n",
    "                    label_id, label_str = 4, \"blinks/blinking\"\n",
    "\n",
    "            # 얼굴 랜드마크 시각화 드로잉\n",
    "            mp_draw.draw_landmarks(\n",
    "                frame,\n",
    "                face_res.multi_face_landmarks[0],\n",
    "                mp_face.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_styles.get_default_face_mesh_tesselation_style()\n",
    "            )\n",
    "            # 눈/입 ROI 박스\n",
    "            def idx_to_pts(idxs):\n",
    "                return [(int(face_landmarks[i].x*width), int(face_landmarks[i].y*height)) for i in idxs]\n",
    "\n",
    "            left_eye_pts = idx_to_pts(LEFT_EYE)\n",
    "            right_eye_pts = idx_to_pts(RIGHT_EYE)\n",
    "            mouth_pts = idx_to_pts([MOUTH_HORZ[0], MOUTH_HORZ[1], MOUTH_VERT[0], MOUTH_VERT[1]])\n",
    "            draw_roi(frame, left_eye_pts, (0,255,0), 2)\n",
    "            draw_roi(frame, right_eye_pts, (0,255,0), 2)\n",
    "            draw_roi(frame, mouth_pts, (255,0,0), 2)\n",
    "\n",
    "            # 손 랜드마크\n",
    "            if hands_res and hands_res.multi_hand_landmarks:\n",
    "                for hlm in hands_res.multi_hand_landmarks:\n",
    "                    mp_draw.draw_landmarks(frame, hlm, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 라벨/지표 텍스트\n",
    "            cv2.putText(frame, f\"Label {label_id}: {label_str}\", (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 200, 255), 2)\n",
    "            if ear_s is not None:\n",
    "                cv2.putText(frame, f\"EAR:{ear_s:.3f}  (low:{EAR_LOW:.3f} high:{EAR_HIGH:.3f})\",\n",
    "                            (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if mar_s is not None:\n",
    "                cv2.putText(frame, f\"MAR:{mar_s:.3f}  (yawn>{MAR_HIGH:.3f})\",\n",
    "                            (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 200, 0), 2)\n",
    "            if yawn_state != YAWN_NONE:\n",
    "                cv2.putText(frame, f\"HandNearMouth: {bool(hand_near)}  YawnFrames:{yawn_count}\",\n",
    "                            (20, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 180, 255), 2)\n",
    "        else:\n",
    "            # 얼굴 미검출 시 기본 라벨 유지(열림 가정)\n",
    "            label_id, label_str = 0, \"eyes_state/open\"\n",
    "            cv2.putText(frame, \"Face not detected\", (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # 프레임 로그\n",
    "        logs.append({\n",
    "            \"frame\": frame_idx,\n",
    "            \"time_sec\": frame_idx / fps,\n",
    "            \"label_id\": label_id,\n",
    "            \"label_name\": label_str,\n",
    "            \"EAR\": float(ear_buf[-1]) if len(ear_buf)>0 else np.nan,\n",
    "            \"MAR\": float(mar_buf[-1]) if len(mar_buf)>0 else np.nan\n",
    "        })\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "print(f\"{time.time()-start:.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "735e6b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video to: labeled_output.mp4\n",
      "Saved per-frame CSV to: per_frame_labels.csv\n"
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "pd.DataFrame(logs).to_csv(\"./output/per_frame_labels.csv\", index=False)\n",
    "print(f\"Saved video to: {OUTPUT_VIDEO}\")\n",
    "print(f\"Saved per-frame CSV to: {OUTPUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
