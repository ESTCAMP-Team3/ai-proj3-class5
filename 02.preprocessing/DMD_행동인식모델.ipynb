{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a301f8c",
   "metadata": {},
   "source": [
    "해당 코드들은 자동화 최적화가 되어 있지 않음\n",
    "\n",
    "아직은 1개씩만 돌려보고 결과를 확인하기 위한 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d983b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91fc970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gA_1_s5_2019-03-14T14;26;17+01;00_rgb_face.mp4', 'gA_2_s5_2019-03-13T09;19;23+01;00_rgb_face.mp4', 'gA_3_s5_2019-03-13T09;36;25+01;00_rgb_face.mp4', 'gA_4_s5_2019-03-13T10;56;52+01;00_rgb_face.mp4', 'gA_5_s5_2019-03-13T09;06;49+01;00_rgb_face.mp4', 'gB_10_s5_2019-03-12T10;35;20+01;00_rgb_face.mp4', 'gB_6_s5_2019-03-13T13;37;11+01;00_rgb_face.mp4', 'gB_7_s5_2019-03-13T13;55;52+01;00_rgb_face.mp4', 'gB_8_s5_2019-03-13T14;10;09+01;00_rgb_face.mp4', 'gB_9_s5_2019-03-07T16;31;48+01;00_rgb_face.mp4', 'gC_11_s5_2019-03-12T09;08;15+01;00_rgb_face.mp4', 'gC_12_s5_2019-03-14T09;56;52+01;00_rgb_face.mp4', 'gC_13_s5_2019-03-12T10;03;00+01;00_rgb_face.mp4', 'gC_14_s5_2019-03-12T09;18;58+01;00_rgb_face.mp4', 'gC_15_s5_2019-03-12T11;03;23+01;00_rgb_face.mp4']\n",
      "['gA_1_s5_2019-03-14T14;26;17+01;00_rgb_ann_drowsiness.json', 'gA_2_s5_2019-03-13T09;19;23+01;00_rgb_ann_drowsiness.json', 'gA_3_s5_2019-03-13T09;36;25+01;00_rgb_ann_drowsiness.json', 'gA_4_s5_2019-03-13T10;56;52+01;00_rgb_ann_drowsiness.json', 'gA_5_s5_2019-03-13T09;06;49+01;00_rgb_ann_drowsiness.json', 'gB_10_s5_2019-03-12T10;35;20+01;00_rgb_ann_drowsiness.json', 'gB_10_s5_2019-03-13T14;17;28+01;00_rgb_ann_drowsiness.json', 'gB_6_s5_2019-03-13T13;37;11+01;00_rgb_ann_drowsiness.json', 'gB_7_s5_2019-03-13T13;55;52+01;00_rgb_ann_drowsiness.json', 'gB_8_s5_2019-03-13T14;10;09+01;00_rgb_ann_drowsiness.json', 'gB_9_s5_2019-03-07T16;31;48+01;00_rgb_ann_drowsiness.json', 'gC_11_s5_2019-03-12T09;08;15+01;00_rgb_ann_drowsiness.json', 'gC_12_s5_2019-03-14T09;56;52+01;00_rgb_ann_drowsiness.json', 'gC_13_s5_2019-03-12T10;03;00+01;00_rgb_ann_drowsiness.json', 'gC_14_s5_2019-03-12T09;18;58+01;00_rgb_ann_drowsiness.json', 'gC_15_s5_2019-03-12T11;03;23+01;00_rgb_ann_drowsiness.json']\n",
      "['.\\\\dmd\\\\gA\\\\1\\\\s5\\\\gA_1_s5_2019-03-14T14;26;17+01;00_rgb_face.mp4', '.\\\\dmd\\\\gA\\\\2\\\\s5\\\\gA_2_s5_2019-03-13T09;19;23+01;00_rgb_face.mp4', '.\\\\dmd\\\\gA\\\\3\\\\s5\\\\gA_3_s5_2019-03-13T09;36;25+01;00_rgb_face.mp4', '.\\\\dmd\\\\gA\\\\4\\\\s5\\\\gA_4_s5_2019-03-13T10;56;52+01;00_rgb_face.mp4', '.\\\\dmd\\\\gA\\\\5\\\\s5\\\\gA_5_s5_2019-03-13T09;06;49+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\10\\\\s5\\\\gB_10_s5_2019-03-12T10;35;20+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\6\\\\s5\\\\gB_6_s5_2019-03-13T13;37;11+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\7\\\\s5\\\\gB_7_s5_2019-03-13T13;55;52+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\8\\\\s5\\\\gB_8_s5_2019-03-13T14;10;09+01;00_rgb_face.mp4', '.\\\\dmd\\\\gB\\\\9\\\\s5\\\\gB_9_s5_2019-03-07T16;31;48+01;00_rgb_face.mp4', '.\\\\dmd\\\\gC\\\\11\\\\s5\\\\gC_11_s5_2019-03-12T09;08;15+01;00_rgb_face.mp4', '.\\\\dmd\\\\gC\\\\12\\\\s5\\\\gC_12_s5_2019-03-14T09;56;52+01;00_rgb_face.mp4', '.\\\\dmd\\\\gC\\\\13\\\\s5\\\\gC_13_s5_2019-03-12T10;03;00+01;00_rgb_face.mp4', '.\\\\dmd\\\\gC\\\\14\\\\s5\\\\gC_14_s5_2019-03-12T09;18;58+01;00_rgb_face.mp4', '.\\\\dmd\\\\gC\\\\15\\\\s5\\\\gC_15_s5_2019-03-12T11;03;23+01;00_rgb_face.mp4']\n",
      "['.\\\\dmd\\\\gA\\\\1\\\\s5\\\\gA_1_s5_2019-03-14T14;26;17+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gA\\\\2\\\\s5\\\\gA_2_s5_2019-03-13T09;19;23+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gA\\\\3\\\\s5\\\\gA_3_s5_2019-03-13T09;36;25+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gA\\\\4\\\\s5\\\\gA_4_s5_2019-03-13T10;56;52+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gA\\\\5\\\\s5\\\\gA_5_s5_2019-03-13T09;06;49+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\10\\\\s5\\\\gB_10_s5_2019-03-12T10;35;20+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\10\\\\s5\\\\gB_10_s5_2019-03-13T14;17;28+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\6\\\\s5\\\\gB_6_s5_2019-03-13T13;37;11+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\7\\\\s5\\\\gB_7_s5_2019-03-13T13;55;52+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\8\\\\s5\\\\gB_8_s5_2019-03-13T14;10;09+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gB\\\\9\\\\s5\\\\gB_9_s5_2019-03-07T16;31;48+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gC\\\\11\\\\s5\\\\gC_11_s5_2019-03-12T09;08;15+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gC\\\\12\\\\s5\\\\gC_12_s5_2019-03-14T09;56;52+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gC\\\\13\\\\s5\\\\gC_13_s5_2019-03-12T10;03;00+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gC\\\\14\\\\s5\\\\gC_14_s5_2019-03-12T09;18;58+01;00_rgb_ann_drowsiness.json', '.\\\\dmd\\\\gC\\\\15\\\\s5\\\\gC_15_s5_2019-03-12T11;03;23+01;00_rgb_ann_drowsiness.json']\n"
     ]
    }
   ],
   "source": [
    "video_file_paths, video_file_names = [], []\n",
    "json_paths, json_names = [], []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\"rgb_face.mp4\"):\n",
    "            video_file_paths.append(os.path.join(root, fname))\n",
    "            video_file_names.append(fname)\n",
    "        if fname.endswith(\"drowsiness.json\"):\n",
    "            json_paths.append(os.path.join(root, fname))\n",
    "            json_names.append(fname)\n",
    "\n",
    "print(video_file_names)\n",
    "print(json_names)\n",
    "print(video_file_paths)\n",
    "print(json_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304910d7",
   "metadata": {},
   "source": [
    "# Mediapipe 영상 labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4618f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIB_SECONDS = 2.0       # 초기 캘리브레이션 구간(초)\n",
    "FPS_FALLBACK = 30.0       # FPS 정보가 없을 때 기본값\n",
    "SMOOTH_WIN = 5            # EAR/MAR 이동평균 윈도우\n",
    "BLINK_MAX_FRAMES = 8      # blink로 볼 수 있는 최대 닫힘 프레임 길이\n",
    "YAWN_MIN_FRAMES = 30      # 하품으로 간주할 최소 프레임 길이\n",
    "HAND_MOUTH_DIST_PX = 80   # 손가락 포인트와 입 중심 간 근접 판정 거리(픽셀)\n",
    "\n",
    "# 상태 머신 임계치 비율 (캘리브레이션 결과에 곱해 사용)\n",
    "EYE_CLOSE_RATIO = 0.85    # 눈감김 임계치: EAR_low = median(EAR_calib)*EYE_CLOSE_RATIO\n",
    "EYE_OPEN_RATIO  = 1.05    # 눈뜸 임계치: EAR_high = median(EAR_calib)*EYE_OPEN_RATIO\n",
    "MOUTH_YAWN_RATIO = 1.25   # 하품 임계치: MAR_high = median(MAR_calib)*MOUTH_YAWN_RATIO\n",
    "\n",
    "# 졸림 지표 윈도우(초)\n",
    "PERCLOS_WIN_SEC = 10.0\n",
    "RATE_WIN_SEC = 20.0\n",
    "\n",
    "# 졸림 임계값(튜닝 가능)\n",
    "PERCLOS_T1, PERCLOS_T2, PERCLOS_T3 = 0.20, 0.40, 0.60\n",
    "YAWN_T1, YAWN_T2, YAWN_T3 = 2.0, 4.0, 6.0           # per minute\n",
    "BLINK_DUR_T1, BLINK_DUR_T2, BLINK_DUR_T3 = 0.25, 0.35, 0.50  # seconds\n",
    "LONG_EC_T1, LONG_EC_T2, LONG_EC_T3 = 0.5, 0.8, 1.0            # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13200b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FaceMesh / Hands 초기화\n",
    "mp_face = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# FaceMesh 눈/입 계산용 랜드마크 인덱스 (MediaPipe FaceMesh)\n",
    "# EAR: (상하 거리 합) / (좌우 거리)  -- 관례적 정의\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]   # [left, top1, top2, right, bottom1, bottom2]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "# MAR: (상하 거리) / (좌우 거리)\n",
    "MOUTH_HORZ = (61, 291)    # 좌우 외측 입꼬리\n",
    "MOUTH_VERT = (13, 14)     # 상하(안쪽 입술 중앙)\n",
    "\n",
    "# 상태 관련 버퍼\n",
    "ear_buf = deque(maxlen=SMOOTH_WIN)\n",
    "mar_buf = deque(maxlen=SMOOTH_WIN)\n",
    "\n",
    "# 상태 머신 상수 (눈 + 하품)\n",
    "EYE_OPEN, EYE_CLOSE, EYE_OPENING, EYE_CLOSING, EYE_BLINK = range(5)\n",
    "YAWN_NONE, YAWN_WITH_HAND, YAWN_WITHOUT_HAND = 0, 1, 2\n",
    "\n",
    "eye_state = EYE_OPEN    # 초기 가정\n",
    "close_count = 0         # 연속 닫힘 프레임 수(블링크 판정용)\n",
    "yawn_state = YAWN_NONE\n",
    "yawn_count = 0\n",
    "\n",
    "# 캘리브레이션 샘플\n",
    "EAR_LOW, EAR_HIGH = 0.18, 0.26\n",
    "MAR_HIGH = 0.60\n",
    "ear_samples = []\n",
    "mar_samples = []\n",
    "\n",
    "# 졸림 지표 계산용 슬라이딩 버퍼 (초 단위 시간축)\n",
    "closed_flags = deque()   # (t, 0/1) — 눈감김 여부\n",
    "blink_events = deque()   # (t, duration_sec)\n",
    "yawn_events  = deque()   # (t,)\n",
    "active_close_start = None  # 현재 진행 중인 eye-closure 시작시간\n",
    "active_blink_start = None  # blink용(짧은 닫힘)\n",
    "\n",
    "# 졸음 단계 색상 팔레트\n",
    "LEVEL_COLOR = {\n",
    "    0: (0, 220, 0),     # alert - green\n",
    "    1: (0, 200, 255),   # mild - orange\n",
    "    2: (0, 128, 255),   # moderate - darker orange\n",
    "    3: (0, 0, 255)      # severe - red\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cdccdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각종 상태 및 라벨을 판별하는 함수들 + ROI\n",
    "def euclid(p1, p2):\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "\n",
    "def eye_aspect_ratio(lm, eye_idx, w, h):\n",
    "    pts = [(lm[i].x*w, lm[i].y*h) for i in eye_idx]\n",
    "    p1, p2, p3, p4, p5, p6 = pts\n",
    "    vertical = (euclid(p2, p6)+euclid(p3, p5))/2.0\n",
    "    horizontal = euclid(p1, p4)+1e-6\n",
    "    return vertical / horizontal\n",
    "\n",
    "def mouth_aspect_ratio(lm, w, h):\n",
    "    L = (lm[MOUTH_HORZ[0]].x*w, lm[MOUTH_HORZ[0]].y*h)\n",
    "    R = (lm[MOUTH_HORZ[1]].x*w, lm[MOUTH_HORZ[1]].y*h)\n",
    "    U = (lm[MOUTH_VERT[0]].x*w, lm[MOUTH_VERT[0]].y*h)\n",
    "    D = (lm[MOUTH_VERT[1]].x*w, lm[MOUTH_VERT[1]].y*h)\n",
    "    horizontal = euclid(L, R) + 1e-6\n",
    "    vertical = euclid(U, D)\n",
    "    return vertical / horizontal, ((L[0]+R[0])/2, (U[1]+D[1])/2)\n",
    "\n",
    "def moving_avg(buf, k):\n",
    "    if len(buf)==0: return None\n",
    "    return float(np.mean(list(buf)[-k:]))\n",
    "\n",
    "def draw_roi(frame, pts, color=(0,255,0), thickness=2):\n",
    "    xs = [p[0] for p in pts]; ys = [p[1] for p in pts]\n",
    "    x1, y1, x2, y2 = int(min(xs)), int(min(ys)), int(max(xs)), int(max(ys))\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "def is_hand_near_mouth(hand_lm_list, mouth_center, max_dist_px, w, h):\n",
    "    if hand_lm_list is None: return False\n",
    "    cx, cy = mouth_center\n",
    "    for hand in hand_lm_list:\n",
    "        for lm in hand.landmark:\n",
    "            px, py = lm.x*w, lm.y*h\n",
    "            if euclid((px, py), (cx, cy)) <= max_dist_px:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def drowsiness_level(perclos, yawn_rate, avg_blink_dur, longest_ec):\n",
    "    # 가장 심한 상태 조건을 우선 판단\n",
    "    # Severe 우선\n",
    "    if (perclos is not None and perclos > PERCLOS_T3) or \\\n",
    "       (yawn_rate is not None and yawn_rate > YAWN_T3) or \\\n",
    "       (longest_ec is not None and longest_ec >= LONG_EC_T3):\n",
    "        return 3, \"severe_drowsy\"\n",
    "    # Moderate\n",
    "    if (perclos is not None and perclos > PERCLOS_T2) or \\\n",
    "       (yawn_rate is not None and yawn_rate > YAWN_T2) or \\\n",
    "       (avg_blink_dur is not None and avg_blink_dur > BLINK_DUR_T2) or \\\n",
    "       (longest_ec is not None and longest_ec >= LONG_EC_T2):\n",
    "        return 2, \"moderate_drowsy\"\n",
    "    # Mild\n",
    "    if (perclos is not None and perclos > PERCLOS_T1) or \\\n",
    "       (yawn_rate is not None and yawn_rate > YAWN_T1) or \\\n",
    "       (avg_blink_dur is not None and avg_blink_dur > BLINK_DUR_T1) or \\\n",
    "       (longest_ec is not None and longest_ec >= LONG_EC_T1):\n",
    "        return 1, \"mild_drowsy\"\n",
    "    # Alert\n",
    "    return 0, \"alert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c58c49",
   "metadata": {},
   "source": [
    "# 행동 라벨링 영상 추출 영역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f1674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0774 sec\n"
     ]
    }
   ],
   "source": [
    "# MediaPipe 컨텍스트\n",
    "# EAR 및 MAR 영역 표시 + 행동상태 라벨링(졸림상태는 X) \n",
    "start = time.time()\n",
    "with mp_face.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    refine_landmarks=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as face_mesh, mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    # 1) 캘리브레이션\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "              # print(\"Webcam frame read failed.\")\n",
    "              break\n",
    "        frame_idx += 1\n",
    "        t_sec = frame_idx / fps\n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face_res = face_mesh.process(rgb)\n",
    "        hand_res = None \n",
    "\n",
    "        if face_res.multi_face_landmarks:\n",
    "            face_landmarks = face_res.multi_face_landmarks[0].landmark\n",
    "            ear_l = eye_aspect_ratio(face_landmarks, LEFT_EYE, width, height)\n",
    "            ear_r = eye_aspect_ratio(face_landmarks, RIGHT_EYE, width, height)\n",
    "            ear = (ear_l + ear_r) / 2.0\n",
    "            mar, mouth_center = mouth_aspect_ratio(face_landmarks, width, height)\n",
    "            ear_samples.append(ear)\n",
    "            mar_samples.append(mar)\n",
    "\n",
    "        if frame_idx >= calib_frames:\n",
    "            break\n",
    "    # 임계치 계산\n",
    "    if len(ear_samples) >= 5:\n",
    "        ear_med = float(np.median(ear_samples))\n",
    "        EAR_LOW  = ear_med * EYE_CLOSE_RATIO\n",
    "        EAR_HIGH = ear_med * EYE_OPEN_RATIO\n",
    "    else:\n",
    "        EAR_LOW, EAR_HIGH = 0.18, 0.26  # 안전 기본값 (얼굴 크기/거리 따라 다를 수 있음)\n",
    "\n",
    "    if len(mar_samples) >= 5:\n",
    "        mar_med = float(np.median(mar_samples))\n",
    "        MAR_HIGH = mar_med * MOUTH_YAWN_RATIO\n",
    "    else:\n",
    "        MAR_HIGH = 0.5  # 안전 기본값\n",
    "    \n",
    "\n",
    "    # 2) 본 처리 루프 (다시 처음부터)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        frame_idx += 1\n",
    "\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face_res = face_mesh.process(rgb)\n",
    "        hands_res = hands.process(rgb)\n",
    "\n",
    "        label_id = 0\n",
    "        label_str = \"eyes_state/open\"\n",
    "\n",
    "        if face_res.multi_face_landmarks:\n",
    "            face_landmarks = face_res.multi_face_landmarks[0].landmark\n",
    "\n",
    "            # EAR/MAR\n",
    "            ear_l = eye_aspect_ratio(face_landmarks, LEFT_EYE, width, height)\n",
    "            ear_r = eye_aspect_ratio(face_landmarks, RIGHT_EYE, width, height)\n",
    "            ear = (ear_l + ear_r) / 2.0\n",
    "            mar, mouth_center = mouth_aspect_ratio(face_landmarks, width, height)\n",
    "\n",
    "            ear_buf.append(ear)\n",
    "            mar_buf.append(mar)\n",
    "            ear_s = moving_avg(ear_buf, SMOOTH_WIN)\n",
    "            mar_s = moving_avg(mar_buf, SMOOTH_WIN)\n",
    "\n",
    "            # 눈 상태 머신 업데이트\n",
    "            prev_state = eye_state\n",
    "            if ear_s is None:\n",
    "                eye_state = EYE_OPEN\n",
    "            else:\n",
    "                # opening/closing은 EAR 변화율로 판정(최근 3프레임) <- 5프레임\n",
    "                if len(ear_buf) >= 3:\n",
    "                    ear_deriv = ear_buf[-1] - ear_buf[-3]\n",
    "                else:\n",
    "                    ear_deriv = 0.0\n",
    "\n",
    "                is_closed = ear_s < EAR_LOW\n",
    "                is_opened = ear_s > EAR_HIGH\n",
    "\n",
    "                if is_closed:\n",
    "                    eye_state = EYE_CLOSE if prev_state == EYE_CLOSE else (EYE_CLOSING if ear_deriv < 0 else EYE_CLOSE)\n",
    "                    close_count += 1\n",
    "                elif is_opened:\n",
    "                    eye_state = EYE_OPEN if prev_state == EYE_OPEN else (EYE_OPENING if ear_deriv > 0 else EYE_OPEN)\n",
    "                    if 0 < close_count <= BLINK_MAX_FRAMES:\n",
    "                        eye_state = EYE_BLINK\n",
    "                    close_count = 0\n",
    "                else:\n",
    "                    eye_state = EYE_OPENING if ear_deriv > 0 else (EYE_CLOSING if ear_deriv < 0 else prev_state)\n",
    "                    if prev_state in (EYE_CLOSE, EYE_CLOSING):\n",
    "                        close_count += 1\n",
    "                    else:\n",
    "                        close_count = 0\n",
    "\n",
    "            # 하품 상태 판별\n",
    "            hand_near = is_hand_near_mouth(\n",
    "                hands_res.multi_hand_landmarks if hands_res else None,\n",
    "                mouth_center, HAND_MOUTH_DIST_PX, width, height\n",
    "            )\n",
    "            if mar_s is not None and mar_s > MAR_HIGH:\n",
    "                yawn_count += 1\n",
    "                yawn_state = YAWN_WITH_HAND if hand_near else YAWN_WITHOUT_HAND\n",
    "            else:\n",
    "                yawn_state = YAWN_NONE\n",
    "                yawn_count = 0\n",
    "\n",
    "            # 최종 라벨 우선순위: 하품 > 눈 상태/눈 깜빡임\n",
    "            if yawn_state != YAWN_NONE and yawn_count >= YAWN_MIN_FRAMES:\n",
    "                if yawn_state == YAWN_WITH_HAND:\n",
    "                    label_id, label_str = 5, \"yawning/Yawning with hand\"\n",
    "                else:\n",
    "                    label_id, label_str = 6, \"yawning/Yawning without hand\"\n",
    "            else:\n",
    "                if eye_state == EYE_OPEN: label_id, label_str = 0, \"eyes_state/open\"\n",
    "                elif eye_state == EYE_CLOSE: label_id, label_str = 1, \"eyes_state/close\"\n",
    "                elif eye_state == EYE_OPENING: label_id, label_str = 2, \"eyes_state/opening\"\n",
    "                elif eye_state == EYE_CLOSING: label_id, label_str = 3, \"eyes_state/closing\"\n",
    "                elif eye_state == EYE_BLINK:   label_id, label_str = 4, \"blinks/blinking\"\n",
    "\n",
    "            # 얼굴 랜드마크 시각화 드로잉\n",
    "            mp_draw.draw_landmarks(\n",
    "                frame,\n",
    "                face_res.multi_face_landmarks[0],\n",
    "                mp_face.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_styles.get_default_face_mesh_tesselation_style()\n",
    "            )\n",
    "            # 눈/입 ROI 박스\n",
    "            def idx_to_pts(idxs):\n",
    "                return [(int(face_landmarks[i].x*width), int(face_landmarks[i].y*height)) for i in idxs]\n",
    "\n",
    "            left_eye_pts = idx_to_pts(LEFT_EYE)\n",
    "            right_eye_pts = idx_to_pts(RIGHT_EYE)\n",
    "            mouth_pts = idx_to_pts([MOUTH_HORZ[0], MOUTH_HORZ[1], MOUTH_VERT[0], MOUTH_VERT[1]])\n",
    "            draw_roi(frame, left_eye_pts, (0,255,0), 2)\n",
    "            draw_roi(frame, right_eye_pts, (0,255,0), 2)\n",
    "            draw_roi(frame, mouth_pts, (255,0,0), 2)\n",
    "\n",
    "            # 손 랜드마크\n",
    "            if hands_res and hands_res.multi_hand_landmarks:\n",
    "                for hlm in hands_res.multi_hand_landmarks:\n",
    "                    mp_draw.draw_landmarks(frame, hlm, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 라벨/지표 텍스트\n",
    "            cv2.putText(frame, f\"Label {label_id}: {label_str}\", (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 200, 255), 2)\n",
    "            if ear_s is not None:\n",
    "                cv2.putText(frame, f\"EAR:{ear_s:.3f}  (low:{EAR_LOW:.3f} high:{EAR_HIGH:.3f})\",\n",
    "                            (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if mar_s is not None:\n",
    "                cv2.putText(frame, f\"MAR:{mar_s:.3f}  (yawn>{MAR_HIGH:.3f})\",\n",
    "                            (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 200, 0), 2)\n",
    "            if yawn_state != YAWN_NONE:\n",
    "                cv2.putText(frame, f\"HandNearMouth: {bool(hand_near)}  YawnFrames:{yawn_count}\",\n",
    "                            (20, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 180, 255), 2)\n",
    "        else:\n",
    "            # 얼굴 미검출 시 기본 라벨 유지(열림 가정)\n",
    "            label_id, label_str = 0, \"eyes_state/open\"\n",
    "            cv2.putText(frame, \"Face not detected\", (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # 프레임 로그\n",
    "        logs.append({\n",
    "            \"frame\": frame_idx,\n",
    "            \"label_id\": label_id,\n",
    "            \"label_name\": label_str,\n",
    "            \"EAR\": float(ear_buf[-1]) if len(ear_buf)>0 else np.nan,\n",
    "            \"MAR\": float(mar_buf[-1]) if len(mar_buf)>0 else np.nan\n",
    "        })\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "pd.DataFrame(logs).to_csv(\"./output/per_frame_labels.csv\", index=False)\n",
    "print(f\"Saved video to: {OUTPUT_VIDEO}\")\n",
    "print(f\"Saved per-frame CSV to: {OUTPUT_CSV}\")\n",
    "print(f\"{time.time()-start:.4f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f7b12",
   "metadata": {},
   "source": [
    "# 졸음 상태 라벨링 영상 추출 영역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ea831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drowsiness_labeling(cap, output, logs, fps, width, height):\n",
    "    with mp_face.FaceMesh(\n",
    "       static_image_mode=False,\n",
    "       refine_landmarks=True,\n",
    "       max_num_faces=1,\n",
    "       min_detection_confidence=0.5,\n",
    "       min_tracking_confidence=0.5\n",
    "       ) as face_mesh, mp_hands.Hands(\n",
    "       static_image_mode=False,\n",
    "       max_num_hands=2,\n",
    "       min_detection_confidence=0.5,\n",
    "       min_tracking_confidence=0.5\n",
    "    ) as hands:\n",
    "       global EAR_HIGH, EAR_LOW, MAR_HIGH\n",
    "\n",
    "       frame_idx = 0\n",
    "       eye_state = EYE_OPEN    # 초기 가정\n",
    "       close_count = 0         # 연속 닫힘 프레임 수(블링크 판정용)\n",
    "       yawn_state = YAWN_NONE\n",
    "       yawn_count = 0\n",
    "\n",
    "       active_close_start = None  # 현재 진행 중인 eye-closure 시작시간\n",
    "       active_blink_start = None  # blink용(짧은 닫힘)\n",
    "\n",
    "       ear_buf.clear()\n",
    "       mar_buf.clear()\n",
    "       blink_events.clear()\n",
    "       yawn_events.clear()\n",
    "       closed_flags.clear()\n",
    "\n",
    "       while True:\n",
    "              ret, frame = cap.read()\n",
    "              if not ret: break\n",
    "              frame_idx += 1\n",
    "              t_sec = frame_idx / fps\n",
    "\n",
    "              rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "              face_res = face_mesh.process(rgb)\n",
    "              hands_res = None  # 조건부로 돌려도 됨(여기선 항상 실행)\n",
    "\n",
    "              label_id = 0\n",
    "              label_str = \"eyes_state/open\"\n",
    "\n",
    "              if face_res.multi_face_landmarks:\n",
    "                     face_landmarks = face_res.multi_face_landmarks[0].landmark\n",
    "\n",
    "                     # EAR/MAR\n",
    "                     ear_l = eye_aspect_ratio(face_landmarks, LEFT_EYE, width, height)\n",
    "                     ear_r = eye_aspect_ratio(face_landmarks, RIGHT_EYE, width, height)\n",
    "                     ear = (ear_l + ear_r) / 2.0\n",
    "                     mar, mouth_center = mouth_aspect_ratio(face_landmarks, width, height)\n",
    "\n",
    "                     ear_buf.append(ear)\n",
    "                     mar_buf.append(mar)\n",
    "                     ear_s = moving_avg(ear_buf, SMOOTH_WIN)\n",
    "                     mar_s = moving_avg(mar_buf, SMOOTH_WIN)\n",
    "\n",
    "                     # 캘리브레이션 샘플 수집\n",
    "                     if frame_idx <= int(CALIB_SECONDS * fps):\n",
    "                            ear_samples.append(ear)\n",
    "                            mar_samples.append(mar)\n",
    "                            if frame_idx == int(CALIB_SECONDS * fps):\n",
    "                                   if len(ear_samples) >= 5:\n",
    "                                          ear_med = float(np.median(ear_samples))\n",
    "                                          EAR_LOW  = ear_med * EYE_CLOSE_RATIO\n",
    "                                          EAR_HIGH = ear_med * EYE_OPEN_RATIO\n",
    "                                   if len(mar_samples) >= 5:\n",
    "                                          mar_med = float(np.median(mar_samples))\n",
    "                                          MAR_HIGH = mar_med * MOUTH_YAWN_RATIO\n",
    "\n",
    "                     # 손-입 근접\n",
    "                     hands_res = hands.process(rgb)\n",
    "                     hand_near = is_hand_near_mouth(\n",
    "                            hands_res.multi_hand_landmarks if hands_res else None,\n",
    "                            mouth_center, HAND_MOUTH_DIST_PX, width, height\n",
    "                     )\n",
    "\n",
    "                     # 눈 상태 머신\n",
    "                     prev_state = eye_state\n",
    "                     if ear_s is None:\n",
    "                            eye_state = EYE_OPEN\n",
    "                     else:\n",
    "                            if len(ear_buf) >= 3:\n",
    "                                   ear_deriv = ear_buf[-1] - ear_buf[-3]\n",
    "                            else:\n",
    "                                   ear_deriv = 0.0\n",
    "\n",
    "                            is_closed = ear_s < EAR_LOW\n",
    "                            is_opened = ear_s > EAR_HIGH\n",
    "\n",
    "                            if is_closed:\n",
    "                                   eye_state = EYE_CLOSE if prev_state == EYE_CLOSE else (EYE_CLOSING if ear_deriv < 0 else EYE_CLOSE)\n",
    "                                   close_count += 1\n",
    "                            elif is_opened:\n",
    "                                   eye_state = EYE_OPEN if prev_state == EYE_OPEN else (EYE_OPENING if ear_deriv > 0 else EYE_OPEN)\n",
    "                                   # blink 판정\n",
    "                                   if 0 < close_count <= BLINK_MAX_FRAMES:\n",
    "                                          eye_state = EYE_BLINK\n",
    "                                   close_count = 0\n",
    "                            else:\n",
    "                                   eye_state = EYE_OPENING if ear_deriv > 0 else (EYE_CLOSING if ear_deriv < 0 else prev_state)\n",
    "                                   if prev_state in (EYE_CLOSE, EYE_CLOSING):\n",
    "                                          close_count += 1\n",
    "                                   else:\n",
    "                                          close_count = 0\n",
    "\n",
    "                     # 하품 상태\n",
    "                     if mar_s is not None and mar_s > MAR_HIGH:\n",
    "                            yawn_count += 1\n",
    "                            yawn_state = YAWN_WITH_HAND if hand_near else YAWN_WITHOUT_HAND\n",
    "                     else:\n",
    "                            yawn_state = YAWN_NONE\n",
    "                            yawn_count = 0\n",
    "\n",
    "                     # 최종 행동 라벨\n",
    "                     if yawn_state != YAWN_NONE and yawn_count >= YAWN_MIN_FRAMES:\n",
    "                            if yawn_state == YAWN_WITH_HAND:\n",
    "                                   label_id, label_str = 5, \"yawning/Yawning with hand\"\n",
    "                            else:\n",
    "                                   label_id, label_str = 6, \"yawning/Yawning without hand\"\n",
    "                     else:\n",
    "                            if   eye_state == EYE_OPEN:    label_id, label_str = 0, \"eyes_state/open\"\n",
    "                            elif eye_state == EYE_CLOSE:   label_id, label_str = 1, \"eyes_state/close\"\n",
    "                            elif eye_state == EYE_OPENING: label_id, label_str = 2, \"eyes_state/opening\"\n",
    "                            elif eye_state == EYE_CLOSING: label_id, label_str = 3, \"eyes_state/closing\"\n",
    "                            elif eye_state == EYE_BLINK:   label_id, label_str = 4, \"blinks/blinking\"\n",
    "\n",
    "                     # ====== 졸림 지표 업데이트 ======\n",
    "                     # A) PERCLOS: 최근 PERCLOS_WIN_SEC 동안 '눈감김' 비율\n",
    "                     closed_flag = 1 if ear_s is not None and ear_s < EAR_LOW else 0\n",
    "                     closed_flags.append((t_sec, closed_flag))\n",
    "                     while closed_flags and (t_sec - closed_flags[0][0] > PERCLOS_WIN_SEC):\n",
    "                            closed_flags.popleft()\n",
    "                     if closed_flags:\n",
    "                            perclos = sum(f for _, f in closed_flags) / float(len(closed_flags))\n",
    "                     else:\n",
    "                            perclos = None\n",
    "\n",
    "                     # B) blink 이벤트 기록(짧은 닫힘)\n",
    "                     # blink가 찍히는 순간(eye_state == EYE_BLINK)에서 duration 계산\n",
    "                     # 간단히: 방금 전까지의 close_count를 duration으로 사용\n",
    "                     if eye_state == EYE_BLINK:\n",
    "                            blink_dur = close_count / fps  # 방금 열림과 함께 close_count가 0으로 초기화되기 전에 계산됨\n",
    "                            blink_events.append((t_sec, blink_dur))\n",
    "                     # 창구 유지\n",
    "                     while blink_events and (t_sec - blink_events[0][0] > RATE_WIN_SEC):\n",
    "                            blink_events.popleft()\n",
    "\n",
    "                     if blink_events:\n",
    "                            blink_rate = len(blink_events) * (60.0 / RATE_WIN_SEC)  # per 20s\n",
    "                            avg_blink_dur = float(np.mean([d for _, d in blink_events]))\n",
    "                     else:\n",
    "                            blink_rate = 0.0\n",
    "                            avg_blink_dur = None\n",
    "\n",
    "                     # C) 긴 eye-closure 감지(연속 close가 길면 이벤트로 기록)\n",
    "                     # close 연속 구간의 시작/끝 추적\n",
    "                     if closed_flag == 1 and active_close_start is None:\n",
    "                            active_close_start = t_sec\n",
    "                     if closed_flag == 0 and active_close_start is not None:\n",
    "                            dur = t_sec - active_close_start\n",
    "                            blink_events.append((t_sec, dur))  # 긴 eye closure도 blink_events에 포함시켜 평균/최대에 반영\n",
    "                            active_close_start = None\n",
    "                     # longest eye closure (최근 RATE_WIN_SEC)\n",
    "                     if blink_events:\n",
    "                            longest_ec = max(d for _, d in blink_events)\n",
    "                     else:\n",
    "                            longest_ec = None\n",
    "\n",
    "                     # D) yawn 이벤트(프레임 지속 충족 시 시점 기록)\n",
    "                     if yawn_state != YAWN_NONE and yawn_count == YAWN_MIN_FRAMES:\n",
    "                            yawn_events.append((t_sec,))\n",
    "                     while yawn_events and (t_sec - yawn_events[0][0] > RATE_WIN_SEC):\n",
    "                            yawn_events.popleft()\n",
    "                     yawn_rate = len(yawn_events) * (60.0 / RATE_WIN_SEC) if yawn_events else 0.0\n",
    "\n",
    "                     # ====== 졸림 단계 산출 ======\n",
    "                     d_level, d_label = drowsiness_level(perclos, yawn_rate, avg_blink_dur, longest_ec)\n",
    "\n",
    "                     # ====== 시각화 ======\n",
    "                     cv2.putText(frame, f\"Label {label_id}: {label_str}\", (20, 40),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 200, 255), 2)\n",
    "                     if ear_s is not None:\n",
    "                            cv2.putText(frame, f\"EAR:{ear_s:.3f} (L:{EAR_LOW:.3f} H:{EAR_HIGH:.3f})\",\n",
    "                                          (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "                     if mar_s is not None:\n",
    "                            cv2.putText(frame, f\"MAR:{mar_s:.3f} (Y>{MAR_HIGH:.3f})\",\n",
    "                                          (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,200,0), 2)\n",
    "\n",
    "                     # 졸림 지표/단계 표시\n",
    "                     c = LEVEL_COLOR[d_level]\n",
    "                     cv2.putText(frame, f\"Drowsiness {d_level}: {d_label}\", (20, 135),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, c, 2)\n",
    "                     cv2.putText(frame, f\"PERCLOS:{(perclos if perclos is not None else np.nan):.2f}  \"\n",
    "                                          f\"Yawn/min:{yawn_rate:.1f}  \"\n",
    "                                          f\"Blink/min:{blink_rate:.1f}  \"\n",
    "                                          f\"AvgBlinkDur:{(avg_blink_dur if avg_blink_dur else np.nan):.2f}s  \"\n",
    "                                          f\"LongestEC:{(longest_ec if longest_ec else np.nan):.2f}s\",\n",
    "                                   (20, 165), cv2.FONT_HERSHEY_SIMPLEX, 0.3, c, 2)\n",
    "\n",
    "              else:\n",
    "                     # 얼굴 미검출 시\n",
    "                     label_id, label_str = 0, \"eyes_state/open\"\n",
    "                     d_level, d_label = 0, \"alert\"\n",
    "                     perclos = None; yawn_rate = 0.0; blink_rate = 0.0; avg_blink_dur = None; longest_ec = None\n",
    "                     cv2.putText(frame, \"Face not detected\", (20, 40),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 2)\n",
    "\n",
    "              # ====== 로그 저장 ======\n",
    "              logs.append({\n",
    "                     \"frame\": frame_idx,\n",
    "                     \"time_sec\": t_sec,\n",
    "                     \"label_id\": label_id,\n",
    "                     \"label_name\": label_str,\n",
    "                     \"EAR\": float(ear_buf[-1]) if len(ear_buf)>0 else np.nan,\n",
    "                     \"MAR\": float(mar_buf[-1]) if len(mar_buf)>0 else np.nan,\n",
    "                     \"yawn_rate_per_min\": float(yawn_rate),\n",
    "                     \"blink_rate_per_min\": float(blink_rate) if face_res.multi_face_landmarks else 0.0,\n",
    "                     \"avg_blink_dur_sec\": float(avg_blink_dur) if avg_blink_dur is not None else np.nan,\n",
    "                     \"longest_eye_closure_sec\": float(longest_ec) if longest_ec is not None else np.nan,\n",
    "                     \"drowsiness_level\": d_level,\n",
    "                     \"drowsiness_label\": d_label\n",
    "              })\n",
    "\n",
    "              output.write(frame)\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2c5935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video: ./output/gA_1_s5_2019-03-14T14;26;17+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gA_1_s5_2019-03-14T14;26;17+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gA_2_s5_2019-03-13T09;19;23+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gA_2_s5_2019-03-13T09;19;23+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gA_3_s5_2019-03-13T09;36;25+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gA_3_s5_2019-03-13T09;36;25+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gA_4_s5_2019-03-13T10;56;52+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gA_4_s5_2019-03-13T10;56;52+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gA_5_s5_2019-03-13T09;06;49+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gA_5_s5_2019-03-13T09;06;49+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gB_10_s5_2019-03-12T10;35;20+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gB_10_s5_2019-03-12T10;35;20+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gB_6_s5_2019-03-13T13;37;11+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gB_6_s5_2019-03-13T13;37;11+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gB_7_s5_2019-03-13T13;55;52+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gB_7_s5_2019-03-13T13;55;52+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gB_8_s5_2019-03-13T14;10;09+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gB_8_s5_2019-03-13T14;10;09+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gB_9_s5_2019-03-07T16;31;48+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gB_9_s5_2019-03-07T16;31;48+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gC_11_s5_2019-03-12T09;08;15+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gC_11_s5_2019-03-12T09;08;15+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gC_12_s5_2019-03-14T09;56;52+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gC_12_s5_2019-03-14T09;56;52+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gC_13_s5_2019-03-12T10;03;00+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gC_13_s5_2019-03-12T10;03;00+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gC_14_s5_2019-03-12T09;18;58+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gC_14_s5_2019-03-12T09;18;58+01;00_rgb_face-labeled.csv\n",
      "Saved video: ./output/gC_15_s5_2019-03-12T11;03;23+01;00_rgb_face-labeled.mp4\n",
      "Saved CSV:   ./output/gC_15_s5_2019-03-12T11;03;23+01;00_rgb_face-labeled.csv\n"
     ]
    }
   ],
   "source": [
    "for video_path, video_name in zip(video_file_paths, video_file_names):\n",
    "       cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "       # 컴퓨터 연결 카메라 사용, 실시간 웹캠 등\n",
    "       #cap = cv2.VideoCapture(CAM_INDEX)   \n",
    "\n",
    "       fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "       if fps <= 1e-2: fps = FPS_FALLBACK\n",
    "       width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "       height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "       total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT)>0 else None\n",
    "\n",
    "       fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "       output_name = video_name.replace(\".mp4\", \"-labeled.mp4\")\n",
    "       out = cv2.VideoWriter(\"./output/\"+output_name, fourcc, fps, (width, height))\n",
    "\n",
    "       logs = []\n",
    "\n",
    "       # 캘리브 구간 프레임 수\n",
    "       calib_frames = int(CALIB_SECONDS * fps)\n",
    "\n",
    "       output_log = drowsiness_labeling(cap, out, logs, fps, width, height)\n",
    "\n",
    "       cap.release()\n",
    "       out.release()\n",
    "       cv2.destroyAllWindows()\n",
    "\n",
    "       pd.DataFrame(logs).to_csv(\"./output/\"+output_name.replace(\".mp4\", \".csv\"), index=False)\n",
    "       print(f\"Saved video: {\"./output/\"+output_name}\")\n",
    "       print(f\"Saved CSV:   {\"./output/\"+output_name.replace(\".mp4\", \".csv\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09df7515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gA_1_s5_2019-03-14T14;26;17+01;00_rgb_face-labeled.csv', 'gA_2_s5_2019-03-13T09;19;23+01;00_rgb_face-labeled.csv', 'gA_3_s5_2019-03-13T09;36;25+01;00_rgb_face-labeled.csv', 'gA_4_s5_2019-03-13T10;56;52+01;00_rgb_face-labeled.csv', 'gA_5_s5_2019-03-13T09;06;49+01;00_rgb_face-labeled.csv', 'gB_10_s5_2019-03-12T10;35;20+01;00_rgb_face-labeled.csv', 'gB_6_s5_2019-03-13T13;37;11+01;00_rgb_face-labeled.csv', 'gB_7_s5_2019-03-13T13;55;52+01;00_rgb_face-labeled.csv', 'gB_8_s5_2019-03-13T14;10;09+01;00_rgb_face-labeled.csv', 'gB_9_s5_2019-03-07T16;31;48+01;00_rgb_face-labeled.csv', 'gC_11_s5_2019-03-12T09;08;15+01;00_rgb_face-labeled.csv', 'gC_12_s5_2019-03-14T09;56;52+01;00_rgb_face-labeled.csv', 'gC_13_s5_2019-03-12T10;03;00+01;00_rgb_face-labeled.csv', 'gC_14_s5_2019-03-12T09;18;58+01;00_rgb_face-labeled.csv', 'gC_15_s5_2019-03-12T11;03;23+01;00_rgb_face-labeled.csv']\n",
      "['./output/gA_1_s5_2019-03-14T14;26;17+01;00_rgb_face-labeled.csv', './output/gA_2_s5_2019-03-13T09;19;23+01;00_rgb_face-labeled.csv', './output/gA_3_s5_2019-03-13T09;36;25+01;00_rgb_face-labeled.csv', './output/gA_4_s5_2019-03-13T10;56;52+01;00_rgb_face-labeled.csv', './output/gA_5_s5_2019-03-13T09;06;49+01;00_rgb_face-labeled.csv', './output/gB_10_s5_2019-03-12T10;35;20+01;00_rgb_face-labeled.csv', './output/gB_6_s5_2019-03-13T13;37;11+01;00_rgb_face-labeled.csv', './output/gB_7_s5_2019-03-13T13;55;52+01;00_rgb_face-labeled.csv', './output/gB_8_s5_2019-03-13T14;10;09+01;00_rgb_face-labeled.csv', './output/gB_9_s5_2019-03-07T16;31;48+01;00_rgb_face-labeled.csv', './output/gC_11_s5_2019-03-12T09;08;15+01;00_rgb_face-labeled.csv', './output/gC_12_s5_2019-03-14T09;56;52+01;00_rgb_face-labeled.csv', './output/gC_13_s5_2019-03-12T10;03;00+01;00_rgb_face-labeled.csv', './output/gC_14_s5_2019-03-12T09;18;58+01;00_rgb_face-labeled.csv', './output/gC_15_s5_2019-03-12T11;03;23+01;00_rgb_face-labeled.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_file_paths, csv_file_names = [], []\n",
    "for root, dirs, files in os.walk(\"./output/\"):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\"-labeled.csv\"):\n",
    "            csv_file_paths.append(os.path.join(root, fname))\n",
    "            csv_file_names.append(fname)\n",
    "            \n",
    "print(csv_file_names)\n",
    "print(csv_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9a06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment화 json 파일 생성 부분\n",
    "# csv 파일을 읽은 후 → drowsiness level에 따라 구역을 segment화 시켜 각 구간의 길이를 계산\n",
    "def make_segments(df, frame_col, label_col, time_col, insert_gaps=True):\n",
    "    d = df.sort_values(by=frame_col).reset_index(drop=True).copy()\n",
    "    d[frame_col] = d[frame_col].astype(int)\n",
    "    d[time_col] = round(d[time_col].astype(float), 3)\n",
    "    d[label_col] = d[label_col].astype(int)\n",
    "    if d.empty: \n",
    "        return []\n",
    "\n",
    "    segs = []\n",
    "    start = int(d.loc[0, frame_col])\n",
    "    start_time = float(d.loc[0, time_col])\n",
    "    prev_frame = start\n",
    "    prev_label = int(d.loc[0, label_col])\n",
    "\n",
    "    for i in range(1, len(d)):\n",
    "        f = int(d.loc[i, frame_col])\n",
    "        t = float(d.loc[i, time_col])\n",
    "        lab = int(d.loc[i, label_col])\n",
    "        # 프레임 갭(누락 프레임) 처리\n",
    "        if f != prev_frame + 1:\n",
    "            segs.append({\"start\": start, \"end\": prev_frame + 1, \"label\": prev_label, \"note\": \"프레임 누락\"})\n",
    "            if insert_gaps and f > prev_frame + 1:\n",
    "                segs.append({\"start\": prev_frame + 1, \"end\": f, \"label\": -1, \"note\": \"무시\"})  # 전이/무시\n",
    "            start, start_time, prev_frame, prev_label = f, t, f, lab\n",
    "            continue\n",
    "        # 라벨 변경 경계\n",
    "        if lab != prev_label:\n",
    "            segs.append({\"start\": start, \"end\": f, \"label\": prev_label, \"note\": f\"시간 표현: {start_time} ~ {t}\"})\n",
    "            start, start_time, prev_label = f, t, lab\n",
    "        prev_frame = f\n",
    "\n",
    "    segs.append({\"start\": start, \"end\": prev_frame + 1, \"label\": prev_label, \"note\": f\"시간 표현: {start_time} ~ {t}\"})\n",
    "    return segs\n",
    "\n",
    "def convert_csv_to_segments_json(csv_path, out_json_path, fps=30):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[[\"frame\", \"time_sec\", \"drowsiness_level\", \"drowsiness_label\"]]\n",
    "\n",
    "    segments = make_segments(df, \"frame\", \"drowsiness_level\", \"time_sec\", insert_gaps=True)\n",
    "    num_frames = int(df[\"frame\"].max()) + 1\n",
    "\n",
    "    friendly = {0:\"정상\", 1:\"의심\", 2:\"주의\", 3:\"위험\", -1:\"무시\"}\n",
    "    class_map = [{\"id\": int(i), \"name\": friendly.get(int(i))} for i in friendly]\n",
    "\n",
    "    payload = {\n",
    "        \"schema_version\": \"1.0\",\n",
    "        \"dataset\": {\n",
    "            \"id\": os.path.splitext(os.path.basename(csv_path))[0],\n",
    "            \"source\": csv_path,\n",
    "            \"fps\": fps,\n",
    "            \"num_frames\": num_frames,\n",
    "            \"timebase\": \"frame\"\n",
    "        },\n",
    "        \"segments\": segments,\n",
    "        \"class_map\": class_map,\n",
    "        \"meta\": {\n",
    "            \"annotator\": \"auto-convert + human-verified\",\n",
    "            \"created_at\": pd.Timestamp.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"comments\": \"half-open [start,end) rule; gaps labeled -1\"\n",
    "        }\n",
    "    }\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7531f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path, video_name, csv_path, csv_name in zip(video_file_paths, video_file_names, csv_file_paths, csv_file_names):\n",
    "    cap = cv2.VideoCapture(video_path)  \n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps <= 1e-2: fps = FPS_FALLBACK\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT)>0 else None\n",
    "\n",
    "    convert_csv_to_segments_json(csv_path=csv_path, \n",
    "                                 out_json_path='./output/segment/'+csv_name.replace('.csv', '.json'),\n",
    "                                 fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcf082",
   "metadata": {},
   "source": [
    "# 튜닝 포인트\n",
    "\n",
    "* 윈도우 길이: PERCLOS_WIN_SEC(기본 30초), RATE_WIN_SEC(기본 60초). 짧게 하면 반응이 빨라지지만 변동성이 커집니다.\n",
    "\n",
    "* 임계값: PERCLOS_T*, YAWN_T*, BLINK_DUR_T*, LONG_EC_T*를 데이터에 맞게 조정하세요.\n",
    "(예: 밤 운전 데이터에서는 PERCLOS 경계가 전체적으로 높게 나올 수 있음)\n",
    "\n",
    "* 성능 최적화: 실시간이 필요하면\n",
    "  * refine_landmarks=False\n",
    "  * 입력 해상도 다운스케일 후 좌표 매핑\n",
    "  * Hands는 하품 후보일 때만 실행(조건부)\n",
    "\n",
    "*  프레임 스킵/검출 주기 적용(앞서 드린 DETECT_EVERY_N 패턴)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e3324",
   "metadata": {},
   "source": [
    "* 신호 품질 & 전처리\n",
    "  * 해상도/ROI 다운스케일: FaceMesh는 480p 또는 얼굴 ROI 기준 긴 변 256~320px로 제한 → 연산↓, 추적 안정↑\n",
    "  * 조명/노이즈: 자동 노출 고정(가능하면), 가우시안(3×3)·샤프닝 약간 → 랜드마크 떨림↓\n",
    "  * 안경/마스크 케이스: EAR 대신 눈꺼풀 거리의 로버스트 비율(눈 구석 점 고정)로 대체 옵션 마련\n",
    "\n",
    "* 피처 튜닝(EAR/MAR 외)\n",
    "  * 고급 PERCLOS: EAR 임계치 하나가 아닌 적응형(중간값±IQR) 임계치로 per-person 보정\n",
    "  * Blink 품질: 닫힘-열림 모두 관측될 때만 이벤트 확정(+최소/최대 지속시간 클램프)\n",
    "  * Yawn 강화: MAR + 입 넓이/세로 비율 + 하악(턱) 이동량(하관 랜드마크 평균 변화) 결합\n",
    "  * Head pose 보조: 고개 떨굼(pitch 상승) 지속 시 drowsy 단계 가중치↑ (PnP로 간단 추정)\n",
    "\n",
    "* 의사결정 로직(안정화)\n",
    "  * 히스테리시스: 졸림 단계는 상승·하강 임계 분리(예: 올라갈 때 PERCLOS 0.40, 내려갈 때 0.35).\n",
    "  * 지속시간 게이팅: 레벨 변동은 연속 T초(예: 5~8s) 유지 시 확정, 아니면 보류.\n",
    "  * 가중 융합: PERCLOS(0.5)·long closure(0.3)·yawn rate(0.2)처럼 가중 합 → 최종 점수→단계 매핑.\n",
    "  * 결측 처리: 얼굴 미검출 시 마지막 유효값 유지(최대 2~3s) 후 ‘unknown’ 표기, 레벨은 완만히 감소.\n",
    "\n",
    "* 윈도우 & 파라미터\n",
    "  * 적응 윈도우: RATE_WIN_SEC 60s 고정 대신 30–90s 가변(최근 변동성 낮으면 길게).\n",
    "  * 초기 캘리브: 2s는 짧음 → 5–10s로 늘리고, 추후 지수 이동 평균으로 천천히 업데이트.\n",
    "  * 개인화: 사용자별 초기 세션에서 EAR/MAR 분포 저장→다음 세션에 로드(프로필별 임계).\n",
    "\n",
    "* 성능 최적화(실시간성)\n",
    "  * 조건부 Hands: (MAR>임계 & 6~12프레임 지속)일 때만 실행.\n",
    "  * DETECT_EVERY_N: FaceDetection 주기 10~15, FaceMesh는 추적 위주.\n",
    "  * 드로잉 최소화: 텍스트/간단 바만 표시, 전체 랜드마크 드로잉은 디버그 전용.\n",
    "  * I/O 최적화: CSV는 주기적 flush(예: 2s마다), VideoWriter는 빠른 코덱(preset 빠르게).\n",
    "\n",
    "* 강건성 & 모니터링\n",
    "  * 품질 스코어: 랜드마크 신뢰(또는 추론 실패율) 로깅 → 나중에 오탐 구간 분석.\n",
    "  * 워치독: 연속 미검출 N초면 “카메라 위치 조정” 경고 프레임 생성.\n",
    "  * 숫자 안정성: NaN/Inf 체크, 분모 0 보호, outlier 클리핑(예: EAR/MAR z-score>4 제거).\n",
    "\n",
    "* 후처리(부드러운 출력)\n",
    "  * 레벨 스무딩: 최종 단계에 모드 필터(3~5프레임) 또는 EMA(α=0.2) 적용 → 점멸 방지.\n",
    "  * 타임라인 바: 프레임 하단에 색 막대로 0~3단계 표시(검수 용이).\n",
    "\n",
    "* 평가/튜닝 워크플로\n",
    "  * 라벨 대비 지표: 프레임 F1 + 경계 오차(초) + 레벨 전이 지연(ms) 기록.\n",
    "  * 그리드 서치: PERCLOS/Yawn/EC 임계를 소범위 격자 탐색→베스트 조합 고정.\n",
    "  * 활성학습: 모델이 불확실(지표 경계 근처) 구간만 표본 추출해 수동 검수→업데이트."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
