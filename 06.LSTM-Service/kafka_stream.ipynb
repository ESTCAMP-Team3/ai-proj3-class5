{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "813165a2",
   "metadata": {},
   "source": [
    "# í˜„ì¬ í•´ê²°í•´ì•¼ í•  ê²ƒ\n",
    "## 1. zolgima-controlì— ìƒˆ start-streamì´ ì•ˆë“¤ì–´ì˜¤ë©´ ì ì‹œ ëŒ€ê¸°\n",
    "## 2. ì—¬ëŸ¬ sessionì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¬ ê²½ìš° ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db51f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import signal\n",
    "import sys\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, deque\n",
    "import tensorflow as tf\n",
    "from confluent_kafka import Producer, Consumer, KafkaException, KafkaError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad04605",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS = [\n",
    "    \"EAR\",\n",
    "    \"MAR\",\n",
    "    \"yawn_rate_per_min\",\n",
    "    \"blink_rate_per_min\",\n",
    "    \"avg_blink_dur_sec\",\n",
    "    \"longest_eye_closure_sec\",\n",
    "]\n",
    "\n",
    "frame_window_size = 150   # í”„ë ˆì„ ë‹¨ìœ„ ìœˆë„ìš°(10ë‹¨ìœ„ ë°˜ì˜¬ë¦¼)\n",
    "hop_frame_size = 30       # í”„ë ˆì„ ë‹¨ìœ„ í™‰(10ë‹¨ìœ„ ë°˜ì˜¬ë¦¼)\n",
    "\n",
    "kafka_conf = {\n",
    "    \"bootstrap.servers\": \"kafka.dongango.com:9094\",\n",
    "    \"group.id\": \"drowsy-session_consumer\",\n",
    "    \"enable.auto.commit\": False,  # ìˆ˜ë™ ì»¤ë°‹\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "    \"session.timeout.ms\": 45000\n",
    "}\n",
    "\n",
    "KAFKA_TOPIC = \"zolgima-control\"\n",
    "\n",
    "window = deque(maxlen=frame_window_size)\n",
    "frame_numbers = deque(maxlen=frame_window_size)\n",
    "CLS_LEVELS = [-1, 0, 1, 2, 3]\n",
    "\n",
    "model = tf.keras.models.load_model(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff85945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_standardizer(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    ì±„ë„ë³„ í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬(í‰ê· /í‘œì¤€í¸ì°¨) ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    # X shape: (N, T, C)\n",
    "    mean = X.reshape(-1, X.shape[-1]).mean(axis=0)\n",
    "    std = X.reshape(-1, X.shape[-1]).std(axis=0)\n",
    "    std = np.where(std < 1e-8, 1.0, std)\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "def apply_standardizer(X: np.ndarray, mean: np.ndarray, std: np.ndarray):\n",
    "    return (X - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ce33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_value(value_dict):\n",
    "    \"\"\"í•„ìš”í•œ featureë§Œ ë½‘ì•„ float ë°°ì—´ë¡œ ë³€í™˜\"\"\"\n",
    "    return [float(value_dict.get(col, np.nan)) for col in FEATURE_COLS]\n",
    "\n",
    "def predict_and_publish(input, frame_no, model, topic):\n",
    "       X = np.array(input).reshape(1, frame_window_size, len(FEATURE_COLS))\n",
    "        \n",
    "       # âœ… í‘œì¤€í™” ì ìš©\n",
    "       mean, std = fit_standardizer(X)\n",
    "       X = apply_standardizer(X, mean, std)\n",
    "\n",
    "       y_pred = model.predict(X, verbose=0)\n",
    "       class_index = int(np.argmax(y_pred, axis=1)[0])   # multi-class softmax ê²°ê³¼\n",
    "       drowsy_level = CLS_LEVELS[class_index]\n",
    "        \n",
    "       result = {\n",
    "              \"frame\": int(frame_no[-1]),  # ë§ˆì§€ë§‰ frame ë²ˆí˜¸\n",
    "              \"drowsy-level\": drowsy_level\n",
    "       }\n",
    "\n",
    "       producer = Producer({\"bootstrap.servers\": kafka_conf[\"bootstrap.servers\"]})\n",
    "        \n",
    "       producer.produce(\n",
    "              topic,\n",
    "              key=\"drowny-lstm-result\",\n",
    "              value=json.dumps(result)\n",
    "       )\n",
    "       producer.flush()\n",
    "       print(\"Published:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15c28cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‚ Subscribed to zolgima-control. Press Ctrl+C to stop.\n",
      "[zolgima-control p0 @ 0] key=start-stream data={'sesstion-id': 'sess-5t1toq4k-mes66sgo', 'session-id': 'sess-5t1toq4k-mes66sgo', 'type': 'jpeg'}\n",
      "[zolgima-control p0 @ 1] key=stop-stream data={'sesstion-id': 'sess-5t1toq4k-mes66sgo', 'session-id': 'sess-5t1toq4k-mes66sgo', 'type': 'jpeg'}\n",
      "[zolgima-control p0 @ 2] key=start-stream data={'session-id': 'sess-7ycx8mo2-mes8s5mb', 'type': 'jpeg'}\n",
      "[zolgima-control p0 @ 3] key=stop-stream data={'session-id': 'sess-7ycx8mo2-mes8s5mb', 'type': 'jpeg'}\n",
      "[zolgima-control p0 @ 4] key=start-stream data={'session-id': 'sess-zwrb7967-mes8vwu6', 'type': 'jpeg'}\n",
      "[zolgima-control p0 @ 5] key=start-stream data={'session-id': 'sess-b3f64s77-mes9356t', 'type': 'jpeg'}\n",
      "ğŸ›‘ Closing consumer...\n"
     ]
    }
   ],
   "source": [
    "running = True\n",
    "all_streams = []\n",
    "started_streams = [0]\n",
    "stopped_streams = []\n",
    "last_started_stream = None\n",
    "\n",
    "def handle_sigint(signum, frame):\n",
    "    global running\n",
    "    running = False\n",
    "\n",
    "c = Consumer(kafka_conf)\n",
    "c.subscribe([KAFKA_TOPIC])\n",
    "IDLE_TIMEOUT_SEC = 5\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_sigint)\n",
    "print(f\"ğŸ‘‚ Subscribed to {KAFKA_TOPIC}. Press Ctrl+C to stop.\")\n",
    "\n",
    "last_msg_ts = time.monotonic()\n",
    "try:\n",
    "       while running:\n",
    "              msg = c.poll(1.0)  # 1ì´ˆ ëŒ€ê¸°\n",
    "              if msg is None:\n",
    "                     if time.monotonic() - last_msg_ts >= IDLE_TIMEOUT_SEC:\n",
    "                       print(\"ğŸ›‘ Idle timeout reached. Stopping session consumer...\")\n",
    "                       break\n",
    "                     continue\n",
    "              if msg.error():\n",
    "                # íŒŒí‹°ì…˜ EOF ë“± ë¬´í•´í•œ ì—ëŸ¬ ì²˜ë¦¬\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    continue\n",
    "                raise KafkaException(msg.error())\n",
    "\n",
    "            # ì •ìƒ ë©”ì‹œì§€ ì²˜ë¦¬\n",
    "              try:\n",
    "                     value = msg.value()\n",
    "                     data = json.loads(value.decode(\"utf-8\")) if value else None\n",
    "                     key = msg.key().decode(\"utf-8\") if msg.key() else None\n",
    "                     print(f\"[{msg.topic()} p{msg.partition()} @ {msg.offset()}] \"\n",
    "                     f\"key={key} data={data}\")\n",
    "                     if key == \"start-stream\":\n",
    "                        started_streams.append(data['session-id'])\n",
    "                     elif key == \"stop-stream\":\n",
    "                        stopped_streams.append(data['session-id'])\n",
    "                     last_started_stream = started_streams[-1]\n",
    "              except json.JSONDecodeError as e:\n",
    "                     print(f\"âš  JSON decode error: {e}; raw={msg.value()!r}\")\n",
    "\n",
    "              if started_streams[-1] == last_started_stream:\n",
    "                      time.sleep(2)\n",
    "                      continue\n",
    "\n",
    "              \n",
    "finally:\n",
    "       print(\"ğŸ›‘ Closing consumer...\")\n",
    "       c.close()\n",
    "last_started_stream = started_streams[-1] if started_streams else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdaf896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sess-3fzhrotz-mev696bb p0 @ 0] key=None data={'topic': 'sess-3fzhrotz-mev696bb', 'frame': 1, 'source_idx': 1, 'timestamp_ms': 1756371636156, 'rel_time_sec': 0.0, 'metrics': {'label_name': 'eyes_state/open', 'EAR': 0.2966610960210734, 'MAR': 0.0022004342718319756, 'yawn_rate_per_min': 0.0, 'blink_rate_per_min': 0.0, 'avg_blink_dur_sec': nan, 'longest_eye_closure_sec': nan, 'ear_left': 0.29837945330365145, 'ear_right': 0.29494273873849536, 'ear_mean': 0.2966610960210734, 'mar': 0.0022004342718319756, 'face_found': True}}\n",
      "ğŸ›‘ Closing consumer...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'window_frame_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m frame_numbers.append(frame_no)\n\u001b[32m     47\u001b[39m window.append([data[\u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m].get(col, np.nan) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m FEATURE_COLS])\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (msg.offset()+\u001b[32m1\u001b[39m) % \u001b[43mwindow_frame_size\u001b[49m == \u001b[32m0\u001b[39m:\n\u001b[32m     49\u001b[39m        \u001b[38;5;28;01mif\u001b[39;00m time.monotonic() - last_msg_ts >= \u001b[32m1\u001b[39m:\n\u001b[32m     50\u001b[39m               frame_df = pd.DataFrame(frames_data)\n",
      "\u001b[31mNameError\u001b[39m: name 'window_frame_size' is not defined"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "window = deque(maxlen=frame_window_size)\n",
    "frame_numbers = deque(maxlen=frame_window_size)\n",
    "\n",
    "session_conf = {\n",
    "        **kafka_conf,\n",
    "        \"group.id\": \"zolgima-session_consumer\",\n",
    "        \"enable.auto.commit\": False,  # ìˆ˜ë™ ì»¤ë°‹\n",
    "        \"auto.offset.reset\": \"earliest\",  # ì²˜ìŒë¶€í„° ì½ê³  ì‹¶ì„ ë•Œ\n",
    "        \"session.timeout.ms\": 45000,\n",
    "}\n",
    "c2 = Consumer(session_conf)\n",
    "\n",
    "c2.subscribe([started_streams[-1]])\n",
    "frames_data = []\n",
    "session_running = True\n",
    "\n",
    "def handle_sigint_session(signum, frame):\n",
    "    global session_running\n",
    "    session_running = False\n",
    "\n",
    "signal.signal(signal.SIGINT, handle_sigint_session)\n",
    "\n",
    "last_msg_ts = time.monotonic()\n",
    "try:\n",
    "       while session_running:\n",
    "              msg = c2.poll(1.0)  # 1ì´ˆ ëŒ€ê¸°\n",
    "              if msg is None:\n",
    "              \n",
    "                continue\n",
    "              if msg.error():\n",
    "                # íŒŒí‹°ì…˜ EOF ë“± ë¬´í•´í•œ ì—ëŸ¬ ì²˜ë¦¬\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    continue\n",
    "                raise KafkaException(msg.error())\n",
    "\n",
    "            # ì •ìƒ ë©”ì‹œì§€ ì²˜ë¦¬\n",
    "              try:\n",
    "                     value = msg.value()\n",
    "                     data = json.loads(value.decode(\"utf-8\")) if value else None\n",
    "                     key = msg.key().decode(\"utf-8\") if msg.key() else None\n",
    "                     print(f\"[{msg.topic()} p{msg.partition()} @ {msg.offset()}] \"\n",
    "                     f\"key={key} data={data}\")\n",
    "                     frames_data.append(data)\n",
    "                     frame_no = int(data.get(\"frame\"))\n",
    "                     frame_numbers.append(frame_no)\n",
    "                     window.append([data['metrics'].get(col, np.nan) for col in FEATURE_COLS])\n",
    "                     if (msg.offset()+1) % frame_window_size == 0:\n",
    "                            if time.monotonic() - last_msg_ts >= 1:\n",
    "                                   frame_df = pd.DataFrame(frames_data)\n",
    "                                   predict_and_publish(window, frame_numbers, model, started_streams[-1]+\"-LSTM\")\n",
    "                                   for _ in range(hop_frame_size):\n",
    "                                          window.popleft()\n",
    "                                          frame_numbers.popleft()\n",
    "                     last_msg_ts = time.monotonic()\n",
    "              except json.JSONDecodeError as e:\n",
    "                     print(f\"âš  JSON decode error: {e}; raw={msg.value()!r}\")\n",
    "finally:\n",
    "       print(\"ğŸ›‘ Closing consumer...\")\n",
    "       c2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
